{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Success vs. Avg Output Tokens\n",
        "\n",
        "1. **Extracts token usage** from result.json files in trial directories\n",
        "2. **Combines with performance data** from difficulty analysis pipeline  \n",
        "3. **Analyzes token efficiency** - tokens per success, output tokens vs success rates\n",
        "4. **Creates visualizations** showing relationships between token usage and performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import ast\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_token_usage():\n",
        "    # TODO: Set path to your terminus2 data directory (output from get_terminus2_runs.py)\n",
        "    base_dir = Path(\"../../../../terminus2_9-17_essential_files\")\n",
        "    token_data = []\n",
        "    \n",
        "    for trial_dir in base_dir.iterdir():\n",
        "        if not trial_dir.is_dir():\n",
        "            continue\n",
        "            \n",
        "        result_file = trial_dir / \"result.json\"\n",
        "        if not result_file.exists():\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            with open(result_file, 'r') as f:\n",
        "                result = json.load(f)\n",
        "            \n",
        "            trial_id = result.get('id')\n",
        "            model_name = result.get('agent_info', {}).get('model_info', {}).get('name')\n",
        "            task_name = result.get('task_name')\n",
        "            \n",
        "            verifier_result = result.get('verifier_result') or {}\n",
        "            reward = verifier_result.get('reward', 0) if isinstance(verifier_result, dict) else 0\n",
        "            \n",
        "            # Token data is in agent_result, not agent_execution\n",
        "            agent_result = result.get('agent_result', {})\n",
        "            if not agent_result:\n",
        "                continue\n",
        "                \n",
        "            input_tokens = agent_result.get('n_input_tokens', 0)\n",
        "            output_tokens = agent_result.get('n_output_tokens', 0)\n",
        "            \n",
        "            # Execution time is in agent_execution\n",
        "            agent_execution = result.get('agent_execution', {})\n",
        "            started_at = agent_execution.get('started_at')\n",
        "            finished_at = agent_execution.get('finished_at')\n",
        "            \n",
        "            execution_time = None\n",
        "            if started_at and finished_at:\n",
        "                start_dt = datetime.fromisoformat(started_at.replace('Z', '+00:00'))\n",
        "                end_dt = datetime.fromisoformat(finished_at.replace('Z', '+00:00'))\n",
        "                execution_time = (end_dt - start_dt).total_seconds()\n",
        "            \n",
        "            token_data.append({\n",
        "                'trial_id': trial_id,\n",
        "                'model_name': model_name,\n",
        "                'task_name': task_name,\n",
        "                'reward': reward,\n",
        "                'success': reward > 0,\n",
        "                'n_input_tokens': input_tokens,\n",
        "                'n_output_tokens': output_tokens,\n",
        "                'total_tokens': input_tokens + output_tokens,\n",
        "                'execution_time_sec': execution_time\n",
        "            })\n",
        "            \n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    return pd.DataFrame(token_data)\n",
        "\n",
        "token_df = extract_token_usage()\n",
        "print(f\"Extracted token data from {len(token_df)} trials\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_provider_from_model(model_name):\n",
        "    if 'claude' in model_name.lower():\n",
        "        return 'Anthropic'\n",
        "    elif 'gpt' in model_name.lower() or 'openai' in model_name.lower():\n",
        "        return 'OpenAI'\n",
        "    elif 'gemini' in model_name.lower():\n",
        "        return 'Google'\n",
        "    elif 'deepseek' in model_name.lower():\n",
        "        return 'DeepSeek'\n",
        "    elif 'kimi' in model_name.lower() or 'moonshot' in model_name.lower():\n",
        "        return 'Moonshot'\n",
        "    elif 'qwen' in model_name.lower():\n",
        "        return 'Alibaba'\n",
        "    elif 'grok' in model_name.lower():\n",
        "        return 'xAI'\n",
        "    elif 'glm' in model_name.lower() or 'zai-org' in model_name.lower():\n",
        "        return 'Zhipu AI'\n",
        "    elif 'llama' in model_name.lower() or 'meta' in model_name.lower():\n",
        "        return 'Meta'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "def process_token_data(token_df):\n",
        "    model_summary = token_df.groupby('model_name').agg({\n",
        "        'n_output_tokens': ['mean', 'std', 'sum'],\n",
        "        'success': ['mean', 'count'],\n",
        "        'execution_time_sec': 'mean'\n",
        "    }).round(3)\n",
        "    \n",
        "    model_summary.columns = [\n",
        "        'avg_output_tokens', 'std_output_tokens', 'total_output_tokens',\n",
        "        'success_rate', 'trial_count', 'avg_execution_time'\n",
        "    ]\n",
        "    \n",
        "    model_summary = model_summary[model_summary['trial_count'] >= 10].copy()\n",
        "    model_summary['provider'] = model_summary.index.map(extract_provider_from_model)\n",
        "    \n",
        "    return model_summary\n",
        "\n",
        "model_summary = process_token_data(token_df)\n",
        "print(f\"Processed data for {len(model_summary)} models with >= 10 trials\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simplify_model_name(model_name):\n",
        "    model_names = {\n",
        "        \"claude-sonnet-4-20250514\": \"Claude Sonnet 4\",\n",
        "        \"claude-opus-4-1-20250805\": \"Claude Opus 4.1\",\n",
        "        \"gpt-5\": \"GPT-5\",\n",
        "        \"gpt-5-mini\": \"GPT-5-Mini\",\n",
        "        \"gpt-5-nano\": \"GPT-5-Nano\",\n",
        "        \"grok-4-0709\": \"Grok 4\",\n",
        "        \"grok-code-fast-1\": \"Grok Code Fast 1\",\n",
        "        \"gemini-2.5-pro\": \"Gemini 2.5 Pro\",\n",
        "        \"gemini-2.5-flash\": \"Gemini 2.5 Flash\",\n",
        "        \"Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\": \"Qwen 3 Coder 480B\",\n",
        "        \"openai/gpt-oss-120b\": \"GPT-OSS 120B\",\n",
        "        \"OpenAI/gpt-oss-20B\": \"GPT-OSS 20B\",\n",
        "        \"moonshotai/Kimi-K2-Instruct-0905\": \"Kimi K2\",\n",
        "        \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\": \"Llama 4 Maverick 17B\",\n",
        "        \"zai-org/GLM-4.5-Air-FP8\": \"GLM 4.5 Air\",\n",
        "        \"deepseek-ai/DeepSeek-V3.1\": \"DeepSeek V3.1\",\n",
        "    }\n",
        "    return model_names.get(model_name, model_name)\n",
        "\n",
        "def create_token_success_plot(model_summary):\n",
        "    print(f\"Data summary:\")\n",
        "    print(f\"Models: {len(model_summary)}\")\n",
        "    print(f\"Output tokens range: {model_summary['avg_output_tokens'].min():.0f} - {model_summary['avg_output_tokens'].max():.0f}\")\n",
        "    print(f\"Success rate range: {model_summary['success_rate'].min():.3f} - {model_summary['success_rate'].max():.3f}\")\n",
        "    \n",
        "    # Check if we have enough variation in the data\n",
        "    if len(model_summary) < 2:\n",
        "        print(\"Not enough models for correlation analysis\")\n",
        "        return None, None\n",
        "        \n",
        "    if model_summary['avg_output_tokens'].std() == 0:\n",
        "        print(\"All models have same output tokens - no correlation possible\")\n",
        "        return None, None\n",
        "        \n",
        "    if model_summary['success_rate'].std() == 0:\n",
        "        print(\"All models have same success rate - no correlation possible\")\n",
        "        return None, None\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    provider_colors = {\n",
        "        'OpenAI': '#d62728',\n",
        "        'Anthropic': '#2ca02c', \n",
        "        'Google': '#ff7f0e',\n",
        "        'xAI': '#bcbd22',\n",
        "        'DeepSeek': '#9467bd',\n",
        "        'Meta': '#8c564b',\n",
        "        'Moonshot': '#e377c2',\n",
        "        'Alibaba': '#7f7f7f',\n",
        "        'Zhipu AI': '#17becf',\n",
        "        'Other': '#1f77b4'\n",
        "    }\n",
        "    \n",
        "    for provider in model_summary['provider'].unique():\n",
        "        provider_data = model_summary[model_summary['provider'] == provider]\n",
        "        ax.scatter(provider_data['avg_output_tokens'], provider_data['success_rate'],\n",
        "                  c=provider_colors.get(provider, '#95A5A6'), \n",
        "                  label=provider, s=100, alpha=0.7, edgecolors='black', linewidth=1)\n",
        "    \n",
        "    x = model_summary['avg_output_tokens']\n",
        "    y = model_summary['success_rate']\n",
        "    \n",
        "    # Try to calculate correlation and trend line with error handling\n",
        "    correlation, p_value = None, None\n",
        "    try:\n",
        "        correlation, p_value = stats.pearsonr(x, y)\n",
        "        \n",
        "        # Only add trend line if correlation is valid\n",
        "        if not np.isnan(correlation):\n",
        "            z = np.polyfit(x, y, 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax.plot(x, p(x), \"r--\", alpha=0.8, linewidth=2, \n",
        "                    label=f'Trend (r={correlation:.3f}, p={p_value:.3f})')\n",
        "    except:\n",
        "        print(\"Could not calculate correlation - data may be constant\")\n",
        "    \n",
        "    # Add model name labels\n",
        "    for _, row in model_summary.iterrows():\n",
        "        display_name = simplify_model_name(row['model_name'])\n",
        "        x_pos = row['avg_output_tokens']\n",
        "        y_pos = row['success_rate']\n",
        "        \n",
        "        ax.annotate(display_name, \n",
        "                   (x_pos, y_pos),\n",
        "                   xytext=(8, 0), textcoords='offset points',\n",
        "                   fontsize=10, ha='left', va='center')\n",
        "    \n",
        "    ax.set_xlabel('Average Output Tokens', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Success Rate', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Success Rate vs Output Tokens', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/success_vs_output_tokens.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig('results/success_vs_output_tokens.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return correlation, p_value\n",
        "\n",
        "Path(\"results\").mkdir(exist_ok=True)\n",
        "correlation, p_value = create_token_success_plot(model_summary)\n",
        "\n",
        "if correlation is not None and p_value is not None:\n",
        "    print(f\"Correlation: {correlation:.3f}, p-value: {p_value:.3f}\")\n",
        "else:\n",
        "    print(\"Could not calculate correlation statistics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_summary.to_csv('results/model_token_summary.csv')\n",
        "\n",
        "provider_summary = model_summary.groupby('provider').agg({\n",
        "    'avg_output_tokens': 'mean',\n",
        "    'success_rate': 'mean',\n",
        "    'trial_count': 'sum',\n",
        "    'avg_execution_time': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "provider_summary.to_csv('results/provider_token_summary.csv')\n",
        "\n",
        "print(\"Saved files:\")\n",
        "print(\"- model_token_summary.csv\")\n",
        "print(\"- provider_token_summary.csv\")\n",
        "print(\"- success_vs_output_tokens.png\")\n",
        "print(\"- success_vs_output_tokens.pdf\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
