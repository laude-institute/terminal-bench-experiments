{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task Difficulty Analysis Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compare:\n",
        "\n",
        "1. MODEL DIFFICULTY (Empirical):\n",
        "- **Model resolve rate** = (models that solve task) / (total models tested)\n",
        "- **Classification**:\n",
        "  - Easy: >= 66.7% of models solve it\n",
        "  - Medium: 33.3%-66.6% of models solve it  \n",
        "  - Hard: < 33.3% of models solve it\n",
        "\n",
        "This gives us empirical difficulty based on actual model performance.\n",
        "\n",
        "2. HUMAN DIFFICULTY (Predicted):\n",
        "- **Human labels** from terminal-bench dataset task definitions\n",
        "- Categories: medium, hard\n",
        "- Based on human assessment of task complexity\n",
        "\n",
        "3. DIFFICULTY MATRIX:\n",
        "Creates a confusion matrix comparing human predictions vs empirical model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import matplotlib.colors as mcolors\n",
        "import toml\n",
        "import yaml\n",
        "\n",
        "# TODO: Set path to your terminus2 data directory (output from get_terminus2_runs.py)\n",
        "TRAJECTORY_DIR = \"\"  # e.g., \"../../../terminus2_9-17_essential_files\"\n",
        "RESULTS_DIR = \"difficulty_analysis_results\"\n",
        "Path(RESULTS_DIR).mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_trial_data(trial_dir):\n",
        "    result_path = trial_dir / \"result.json\"\n",
        "    if not result_path.exists():\n",
        "        return None\n",
        "        \n",
        "    try:\n",
        "        with open(result_path) as f:\n",
        "            result = json.load(f)\n",
        "        \n",
        "        verifier_result = result.get('verifier_result') or {}\n",
        "        reward = verifier_result.get('reward', 0) if isinstance(verifier_result, dict) else 0\n",
        "        \n",
        "        data = {\n",
        "            'trial_id': result.get('id'),\n",
        "            'trial_name': result.get('trial_name'),\n",
        "            'model_name': result.get('agent_info', {}).get('model_info', {}).get('name'),\n",
        "            'agent_name': result.get('agent_info', {}).get('name'),\n",
        "            'task_name': result.get('task_name'),\n",
        "            'reward': reward,\n",
        "            'success': reward > 0,\n",
        "            'trial_uri': result.get('trial_uri'),\n",
        "            'created_at': result.get('created_at'),\n",
        "        }\n",
        "        \n",
        "        for phase in ['setup', 'execution']:\n",
        "            phase_data = result.get(f'agent_{phase}', {})\n",
        "            start = phase_data.get('started_at')\n",
        "            end = phase_data.get('finished_at')\n",
        "            if start and end:\n",
        "                start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n",
        "                end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))\n",
        "                data[f'agent_{phase}_time_sec'] = (end_dt - start_dt).total_seconds()\n",
        "            else:\n",
        "                data[f'agent_{phase}_time_sec'] = None\n",
        "        \n",
        "        return data\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_all_trials(trajectories_dir=TRAJECTORY_DIR, output_dir=RESULTS_DIR):\n",
        "    trajectories_path = Path(trajectories_dir)\n",
        "    if not trajectories_path.exists():\n",
        "        return {}, pd.DataFrame()\n",
        "    \n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    cache_path = output_path / \"trials_cache.json\"\n",
        "    trials_dict = {}\n",
        "    if cache_path.exists():\n",
        "        with open(cache_path) as f:\n",
        "            trials_dict = json.load(f)\n",
        "    \n",
        "    trial_dirs = [d for d in trajectories_path.iterdir() if d.is_dir()]\n",
        "    new_count = 0\n",
        "    \n",
        "    for trial_dir in trial_dirs:\n",
        "        trial_id = trial_dir.name\n",
        "        if trial_id not in trials_dict:\n",
        "            trial_data = extract_trial_data(trial_dir)\n",
        "            if trial_data:\n",
        "                trials_dict[trial_id] = trial_data\n",
        "                new_count += 1\n",
        "    \n",
        "    if new_count > 0:\n",
        "        with open(cache_path, 'w') as f:\n",
        "            json.dump(trials_dict, f, indent=2)\n",
        "    \n",
        "    return trials_dict, pd.DataFrame(list(trials_dict.values()))\n",
        "\n",
        "trials_dict, trials_df = extract_all_trials()\n",
        "trials_df.to_csv(f\"{RESULTS_DIR}/trials_raw.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_model_task_performance(trials_df):\n",
        "    trials_df = trials_df.drop_duplicates('trial_id')\n",
        "    \n",
        "    grouped = trials_df.groupby(['model_name', 'task_name']).agg({\n",
        "        'trial_id': ['count', list],\n",
        "        'trial_name': list,\n",
        "        'success': ['sum', list],\n",
        "        'agent_execution_time_sec': list,\n",
        "        'agent_setup_time_sec': list,\n",
        "    }).reset_index()\n",
        "    \n",
        "    grouped.columns = ['model_name', 'task_name', 'total_trials', 'trial_ids', 'trial_names',\n",
        "                      'successful_trials', 'success_list', 'execution_times_sec', 'setup_times_sec']\n",
        "    \n",
        "    grouped['success_rate'] = grouped['successful_trials'] / grouped['total_trials']\n",
        "    grouped['resolves_task'] = grouped['successful_trials'] > (grouped['total_trials'] / 2)\n",
        "    \n",
        "    grouped['successful_trial_ids'] = grouped.apply(\n",
        "        lambda row: [tid for tid, success in zip(row['trial_ids'], row['success_list']) if success], \n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    grouped = grouped.drop(['success_list'], axis=1)\n",
        "    return grouped\n",
        "\n",
        "performance_df = calculate_model_task_performance(trials_df)\n",
        "performance_df.to_csv(f\"{RESULTS_DIR}/model_task_performance.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_task_resolution(performance_df):\n",
        "    task_groups = performance_df.groupby('task_name').agg({\n",
        "        'model_name': list,\n",
        "        'resolves_task': ['count', 'sum', list],\n",
        "    }).reset_index()\n",
        "    \n",
        "    task_groups.columns = ['task_name', 'all_models', 'total_models_tested', 'models_that_resolve', 'resolve_list']\n",
        "    \n",
        "    task_groups['resolving_models'] = task_groups.apply(\n",
        "        lambda row: [model for model, resolves in zip(row['all_models'], row['resolve_list']) if resolves],\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    task_groups['non_resolving_models'] = task_groups.apply(\n",
        "        lambda row: [model for model, resolves in zip(row['all_models'], row['resolve_list']) if not resolves],\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    task_groups['model_resolve_rate'] = task_groups['models_that_resolve'] / task_groups['total_models_tested']\n",
        "    task_groups = task_groups.drop(['all_models', 'resolve_list'], axis=1)\n",
        "    \n",
        "    return task_groups\n",
        "\n",
        "resolution_df = calculate_task_resolution(performance_df)\n",
        "resolution_df.to_csv(f\"{RESULTS_DIR}/task_resolution.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_task_difficulty(resolution_df):\n",
        "    def classify_difficulty(resolve_rate):\n",
        "        if resolve_rate >= 0.667:\n",
        "            return 'easy'\n",
        "        elif resolve_rate >= 0.333:\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'hard'\n",
        "    \n",
        "    difficulty_df = resolution_df.copy()\n",
        "    difficulty_df['model_difficulty'] = difficulty_df['model_resolve_rate'].apply(classify_difficulty)\n",
        "    difficulty_df['human_difficulty'] = None\n",
        "    \n",
        "    cols = ['task_name', 'model_resolve_rate', 'model_difficulty', 'human_difficulty', \n",
        "            'total_models_tested', 'models_that_resolve', 'resolving_models', 'non_resolving_models']\n",
        "    difficulty_df = difficulty_df[cols]\n",
        "    \n",
        "    return difficulty_df\n",
        "\n",
        "difficulty_df = classify_task_difficulty(resolution_df)\n",
        "difficulty_df.to_csv(f\"{RESULTS_DIR}/task_difficulty.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_human_difficulty_labels():\n",
        "    human_labels = {}\n",
        "    \n",
        "    # TODO: Set path to your terminal-bench-2.0-dataset repository\n",
        "    terminal_bench_2_path = Path.home() / \"path/to/terminal-bench-2.0-dataset\" / \"tasks\"\n",
        "    if terminal_bench_2_path.exists():\n",
        "        for task_dir in terminal_bench_2_path.iterdir():\n",
        "            if not task_dir.is_dir():\n",
        "                continue\n",
        "            task_toml_path = task_dir / \"task.toml\"\n",
        "            if not task_toml_path.exists():\n",
        "                continue\n",
        "            try:\n",
        "                with open(task_toml_path, 'r') as f:\n",
        "                    task_config = toml.load(f)\n",
        "                task_name = task_dir.name\n",
        "                difficulty = task_config.get('metadata', {}).get('difficulty', 'unknown')\n",
        "                if difficulty != 'unknown':\n",
        "                    human_labels[task_name] = difficulty.lower()\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # TODO: Set path to your terminal-bench repository\n",
        "    terminal_bench_path = Path.home() / \"path/to/terminal-bench\" / \"tasks\"\n",
        "    if terminal_bench_path.exists():\n",
        "        for task_dir in terminal_bench_path.iterdir():\n",
        "            if not task_dir.is_dir():\n",
        "                continue\n",
        "            task_yaml_path = task_dir / \"task.yaml\"\n",
        "            if not task_yaml_path.exists():\n",
        "                continue\n",
        "            try:\n",
        "                with open(task_yaml_path, 'r') as f:\n",
        "                    task_config = yaml.safe_load(f)\n",
        "                task_name = task_dir.name\n",
        "                difficulty = task_config.get('difficulty', 'unknown')\n",
        "                if difficulty != 'unknown' and task_name not in human_labels:\n",
        "                    human_labels[task_name] = difficulty.lower()\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return human_labels\n",
        "\n",
        "def create_confusion_matrix(difficulty_df):\n",
        "    human_labels = load_human_difficulty_labels()\n",
        "    \n",
        "    def get_human_difficulty(task_name):\n",
        "        return human_labels.get(task_name, 'unknown')\n",
        "    \n",
        "    difficulty_df['human_difficulty'] = difficulty_df['task_name'].apply(get_human_difficulty)\n",
        "    difficulty_df = difficulty_df[difficulty_df['human_difficulty'] != 'unknown']\n",
        "    \n",
        "    if len(difficulty_df) == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    confusion = pd.crosstab(\n",
        "        difficulty_df['human_difficulty'], \n",
        "        difficulty_df['model_difficulty'],\n",
        "        margins=False\n",
        "    )\n",
        "    \n",
        "    human_categories = ['hard', 'medium']\n",
        "    empirical_categories = ['easy', 'medium', 'hard']\n",
        "    \n",
        "    confusion = confusion.reindex(\n",
        "        index=human_categories, \n",
        "        columns=empirical_categories, \n",
        "        fill_value=0\n",
        "    )\n",
        "    \n",
        "    return confusion\n",
        "\n",
        "def plot_compact_heatmap(confusion_matrix, output_base, normalize=False):\n",
        "    plt.rcParams['figure.dpi'] = 300\n",
        "    plt.rcParams['savefig.dpi'] = 300\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(4.2, 3.6))\n",
        "    \n",
        "    if normalize:\n",
        "        confusion_normalized = confusion_matrix.div(confusion_matrix.sum(axis=1), axis=0) * 100\n",
        "        plot_data = confusion_normalized.values\n",
        "        colorbar_label = 'Percentage of Tasks'\n",
        "        value_format = '.1f'\n",
        "        value_suffix = '%'\n",
        "    else:\n",
        "        plot_data = confusion_matrix.values\n",
        "        colorbar_label = 'Number of Tasks'\n",
        "        value_format = 'd'\n",
        "        value_suffix = ''\n",
        "    \n",
        "    colors = ['#f7f7f7', '#c6dbef', '#6baed6', '#3182bd', '#08519c']\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_blues', colors, N=256)\n",
        "    \n",
        "    im = ax.imshow(plot_data, cmap=cmap, aspect='equal')\n",
        "    \n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.5, pad=0.05)\n",
        "    cbar.set_label(colorbar_label, rotation=90, labelpad=10, fontsize=10)\n",
        "    cbar.ax.tick_params(labelsize=9)\n",
        "    \n",
        "    ax.set_xticks(range(len(confusion_matrix.columns)))\n",
        "    ax.set_yticks(range(len(confusion_matrix.index)))\n",
        "    ax.set_xticklabels([col.capitalize() for col in confusion_matrix.columns], fontsize=11)\n",
        "    ax.set_yticklabels([idx.capitalize() for idx in confusion_matrix.index], fontsize=11)\n",
        "    \n",
        "    for i in range(len(confusion_matrix.index)):\n",
        "        for j in range(len(confusion_matrix.columns)):\n",
        "            if normalize:\n",
        "                value = confusion_normalized.iloc[i, j]\n",
        "                display_value = f\"{value:{value_format}}{value_suffix}\"\n",
        "            else:\n",
        "                value = confusion_matrix.iloc[i, j]\n",
        "                display_value = f\"{value:{value_format}}{value_suffix}\"\n",
        "            \n",
        "            text_color = 'black' if value < plot_data.max() * 0.6 else 'white'\n",
        "            ax.text(j, i, display_value, \n",
        "                   ha=\"center\", va=\"center\", \n",
        "                   color=text_color, \n",
        "                   fontsize=12, \n",
        "                   fontweight='bold')\n",
        "    \n",
        "    plt.title('Task Difficulty Matrix', fontsize=12, pad=15, fontweight='bold')\n",
        "    plt.xlabel('Empirical Difficulty', fontsize=10, fontweight='bold', labelpad=8)\n",
        "    plt.ylabel('Human-Predicted Difficulty', fontsize=10, fontweight='bold', labelpad=8)\n",
        "    \n",
        "    ax.set_xticks(np.arange(len(confusion_matrix.columns)) - 0.5, minor=True)\n",
        "    ax.set_yticks(np.arange(len(confusion_matrix.index)) - 0.5, minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=2)\n",
        "    ax.tick_params(which=\"minor\", size=0)\n",
        "    \n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "    \n",
        "    plt.tight_layout(pad=0.8)\n",
        "    \n",
        "    output_file = f\"{output_base}.png\"\n",
        "    plt.savefig(output_file, \n",
        "               dpi=300,\n",
        "               bbox_inches='tight', \n",
        "               pad_inches=0.05,\n",
        "               facecolor='white',\n",
        "               edgecolor='none')\n",
        "    \n",
        "    plt.show()\n",
        "    return confusion_matrix\n",
        "\n",
        "# Update CSV with human difficulty labels\n",
        "def load_human_labels_for_csv():\n",
        "    human_labels = {}\n",
        "    \n",
        "    # TODO: Set path to your terminal-bench-2.0-dataset repository\n",
        "    path_2_0 = Path.home() / \"path/to/terminal-bench-2.0-dataset\" / \"tasks\"\n",
        "    if path_2_0.exists():\n",
        "        for task_dir in path_2_0.iterdir():\n",
        "            if not task_dir.is_dir():\n",
        "                continue\n",
        "            task_toml = task_dir / \"task.toml\"\n",
        "            if not task_toml.exists():\n",
        "                continue\n",
        "            try:\n",
        "                with open(task_toml, 'r') as f:\n",
        "                    config = toml.load(f)\n",
        "                difficulty = config.get('metadata', {}).get('difficulty', 'unknown')\n",
        "                if difficulty != 'unknown':\n",
        "                    human_labels[task_dir.name] = difficulty.lower()\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # TODO: Set path to your terminal-bench repository\n",
        "    path_orig = Path.home() / \"path/to/terminal-bench\" / \"tasks\"\n",
        "    if path_orig.exists():\n",
        "        for task_dir in path_orig.iterdir():\n",
        "            if not task_dir.is_dir():\n",
        "                continue\n",
        "            task_yaml = task_dir / \"task.yaml\"\n",
        "            if not task_yaml.exists():\n",
        "                continue\n",
        "            try:\n",
        "                with open(task_yaml, 'r') as f:\n",
        "                    config = yaml.safe_load(f)\n",
        "                task_name = task_dir.name\n",
        "                difficulty = config.get('difficulty', 'unknown')\n",
        "                if difficulty != 'unknown' and task_name not in human_labels:\n",
        "                    human_labels[task_name] = difficulty.lower()\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return human_labels\n",
        "\n",
        "# Update difficulty_df with human labels\n",
        "human_labels_for_csv = load_human_labels_for_csv()\n",
        "difficulty_df['human_difficulty'] = difficulty_df['task_name'].map(human_labels_for_csv)\n",
        "difficulty_df.to_csv(f\"{RESULTS_DIR}/task_difficulty.csv\", index=False)\n",
        "\n",
        "confusion = create_confusion_matrix(difficulty_df)\n",
        "if len(confusion) > 0:\n",
        "    plot_compact_heatmap(confusion, f\"{RESULTS_DIR}/difficulty_heatmap_normalized\", normalize=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
